name: &name efficient_mixit_sparsityloss23_conformer_snr

# use pretrained model trained without sparsity loss
student_init_param: ../model/wsjmix/mixit/efficient_mixit_conformer_snr/checkpoint_epoch300/separator.pth

max_epoch: 400
batch_size: 32 # input MoM has size of batch_size//2
batch_size_valid: 4
seed: 0
max_grad_norm: 5

keep_nbest_models: 10
best_model_criterion: [student_msi, max]
save_checkpoint_interval: 50

algo: mixit
algo_conf:
  generalized: true
  ensure_mixconsis: true
  normalize: true
  efficient_mixit: true
  sparsity_loss_weight: 23.0

loss: snr
loss_conf:
  snr_max: 30
  solve_perm: true
  only_denominator: false
  threshold_with: reference

# dataset-related
dataset: fuss
dataset_conf:
  num_train_data: null
  num_valid_data: null
  params:
    normalization: false
workers: 8

# now we only support stft
frontend: stft
frontend_conf:
  n_fft: &n_fft 512
  win_length: 400
  hop_length: 160
  window: hann

# separator conf
separator: ConformerSeparator
separator_conf:
  input_dim: 257
  num_spk: 8
  predict_noise: false # total 4 masks
  adim: 256
  aheads: 4
  layers: 16
  linear_units: 1024
  positionwise_layer_type: linear
  positionwise_conv_kernel_size: 1
  normalize_before: true
  dropout_rate: 0.0
  input_layer: linear
  positional_dropout_rate: 0.0
  attention_dropout_rate: 0.0
  nonlinear: sigmoid
  conformer_pos_enc_layer_type: rel_pos
  conformer_self_attn_layer_type: rel_selfattn
  conformer_activation_type: swish
  use_macaron_style_in_conformer: true
  use_cnn_in_conformer: true
  conformer_enc_kernel_size: 33
  n_groups: 8

# optimizer conf
optimizer: adamw
optimizer_conf:
  lr: &lr 4.0e-04
  weight_decay: 1.0e-02
  eps: 1.e-08

# scheduler conf
scheduler: warmup
scheduler_conf:
  mode: steplr
  lr: *lr
  patience: 3
  factor: 0.98
  warmup_steps: 5000
  min_lr: 2.0e-05
  decay_start_epoch: 100

# wandb conf
use_wandb: false
wandb:
  project: selfremixing
  name: *name

# mixed precision
amp_params:
  enabled: false
  init_scale: 1280
